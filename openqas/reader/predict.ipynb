{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Dense, Activation\n",
    "from keras.layers import LSTM, Bidirectional, Input\n",
    "from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-levenshtein in /home/gurupunskill/anaconda3/lib/python3.6/site-packages (0.12.0)\n",
      "Requirement already satisfied: setuptools in /home/gurupunskill/anaconda3/lib/python3.6/site-packages (from python-levenshtein) (40.8.0)\n",
      "Requirement already satisfied: fuzzywuzzy in /home/gurupunskill/anaconda3/lib/python3.6/site-packages (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install python-levenshtein\n",
    "!pip install fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz, StringMatcher\n",
    "\n",
    "class WordEmbeddings:\n",
    "    \"\"\"\n",
    "    Module to load and handle the GloVe Word Embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocabulary = set()\n",
    "        self.word_to_vec = {}\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "    \n",
    "    def load_glove(self, path):\n",
    "        \"\"\"\n",
    "        Loads a pretrained GloVe model\n",
    "        Expects a path to a GloVe pretrained word embeddings file\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path, 'r') as file:\n",
    "            for line in file:              \n",
    "                line = line.strip().split()\n",
    "                self.vocabulary.add(line[0])\n",
    "                self.word_to_vec[line[0]] = np.array(line[1:], dtype='float64')\n",
    "    \n",
    "            for x,y in enumerate(sorted(self.vocabulary)):\n",
    "                self.word_to_index[y] = x+1\n",
    "                self.index_to_word[x+1] = y\n",
    "        \n",
    "        self.EMBEDDING_DIM = len(self.word_to_vec['the'])\n",
    "        print(len(self.word_to_vec))\n",
    "        print(self.EMBEDDING_DIM)\n",
    "        \n",
    "    def get_matrix(self):\n",
    "        embedding_matrix = np.zeros((len(self.word_to_index) + 1, self.EMBEDDING_DIM))\n",
    "        for word, i in self.word_to_index.items():\n",
    "            embedding_vector = self.word_to_vec[word]\n",
    "            if(embedding_vector is not None):\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        print(embedding_matrix)\n",
    "        return embedding_matrix\n",
    "\n",
    "    def in_vocab(self, word):\n",
    "        \"\"\"\n",
    "        Checks if a word is present in the vocabulary\n",
    "        \"\"\"\n",
    "        return (word in self.vocabulary)\n",
    "\n",
    "    def autocorrect(self, wrong_word):\n",
    "        \"\"\"\n",
    "        Attempts to map a wrongly spelt word to the closest one present in the vocabulary.\n",
    "        THIS IS NOT COSINE SIMILARITY. THIS IS AUTOCORRECT.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.in_vocab(wrong_word):\n",
    "            return wrong_word\n",
    "\n",
    "        closest_ratio = 0.0\n",
    "        closest_word = None\n",
    "        for word in self.vocabulary:\n",
    "            if fuzz.ratio(word,wrong_word) > closest_ratio :\n",
    "                closest_word = word\n",
    "                closest_ratio = fuzz.ratio(word,wrong_word)\n",
    "        return closest_word\n",
    "\n",
    "    def similarity(self, word_1, word_2):\n",
    "        \"\"\"\n",
    "        Returns the cosine similarity of word_1 and word_2\n",
    "        \"\"\"\n",
    "        \n",
    "        assert (self.in_vocab(word_1) and self.in_vocab(word_2))\n",
    "\n",
    "        u = self.word_to_vec[word_1]\n",
    "        v = self.word_to_vec[word_2]\n",
    "\n",
    "        dot = np.sum(u * v)\n",
    "        norm_u = np.sqrt(np.sum(u ** 2))\n",
    "        norm_v = np.sqrt(np.sum(v ** 2))\n",
    "        cosine_similarity = dot / (norm_u * norm_v)\n",
    "\n",
    "        return cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = WordEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400001\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "glove_embeddings.load_glove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         0.         0.        ...  0.         0.         0.       ]\n",
      " [ 0.38472    0.49351    0.49096   ...  0.026263   0.39052    0.52217  ]\n",
      " [ 0.22657    0.64651    0.84828   ...  0.54712    0.7697     0.35075  ]\n",
      " ...\n",
      " [ 0.14335    0.557     -0.68806   ...  0.10501   -0.49575    0.39039  ]\n",
      " [-0.036419  -0.63433   -0.26185   ...  0.25043    0.21037    0.75933  ]\n",
      " [ 0.32008    0.21479   -0.036466  ...  0.088318   0.11623    0.0020262]]\n"
     ]
    }
   ],
   "source": [
    "matrix = glove_embeddings.get_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/gurupunskill/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/gurupunskill/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3368: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "context_input = Input(shape=(None,),dtype='int32', name='context_input')\n",
    "x= Embedding(input_dim=400002, output_dim=100, weights=[matrix], trainable=False)(context_input)\n",
    "hidden_layer = Bidirectional(LSTM(128, return_state=False, return_sequences=True),merge_mode='concat')(x)\n",
    "drop_1 = Dropout(0.5)(hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_input = Input(shape=(None, ), dtype='int32', name='context_input')\n",
    "x= Embedding(input_dim=400002, output_dim=100, weights=[matrix], \n",
    "               trainable=False)(context_input)\n",
    "hidden_layer = Bidirectional(LSTM(128, return_state=False, return_sequences=True),merge_mode='concat')(x)\n",
    "drop_2 = Dropout(0.5)(hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('context.h5', 'r') as hf:\n",
    "    context_array = hf['context'][:]\n",
    "with h5py.File('questions.h5', 'r') as hf:\n",
    "    question_array = hf['questions'][:]\n",
    "with h5py.File('begin.h5', 'r') as hf:\n",
    "    begin_span = hf['begin'][:]\n",
    "with h5py.File('end.h5', 'r') as hf:\n",
    "    end_span = hf['end'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "context_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    40000200    context_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    40000200    context_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 256)    234496      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 256)    234496      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 256)    0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 256)    0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 256)    0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 128)          394240      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3126)         403254      bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 3136)         404544      bidirectional_3[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 81,671,430\n",
      "Trainable params: 1,671,030\n",
      "Non-trainable params: 80,000,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_span_begin = np.amax(begin_span)\n",
    "max_span_end = np.amax(end_span)\n",
    "batch = 100\n",
    "# slice of data to be used as one epoch training on full data is expensive\n",
    "slce = 100\n",
    "merge_layer = concatenate([drop_1, drop_2], axis=1)\n",
    "biLSTM = Bidirectional(LSTM(128, implementation=2), merge_mode='mul')(merge_layer)\n",
    "drop_3 =  Dropout(0.5)(biLSTM)\n",
    "softmax_1 = Dense(max_span_begin, activation='softmax')(biLSTM)\n",
    "softmax_2 = Dense(max_span_end, activation='softmax')(biLSTM)\n",
    "\n",
    "model = Model(inputs=[context_input, ques_input], outputs=[softmax_1, softmax_2])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model.fit([context_array[:slce], question_array[:slce]],\n",
    "                        [begin_span[:slce], end_span[:slce]], verbose=2,\n",
    "                         batch_size=batch, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
