{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM, Input\n",
    "from keras.models import Model\n",
    "\n",
    "MAX_WORDS = 50\n",
    "\n",
    "def GetGlove(glove_file):\n",
    "    with open(glove_file, 'r') as file:\n",
    "        words = set()\n",
    "        word_to_vec, word_to_index, index_to_word = {},{},{}\n",
    "\n",
    "        for line in file:\n",
    "            line = line.strip().split()\n",
    "            words.add(line[0])\n",
    "            word_to_vec[line[0]] = np.array(line[1:], dtype='float64')\n",
    "    \n",
    "    for x,y in enumerate(sorted(words)):\n",
    "        word_to_index[y] = x\n",
    "        index_to_word[x] = y\n",
    "        \n",
    "    return {'word_to_vec': word_to_vec, 'word_to_index': word_to_index, 'index_to_word': index_to_word}\n",
    "\n",
    "def QuestionIndices(Q, word_to_vec, word_to_index):\n",
    "    x_index = np.zeros((len(Q), MAX_WORDS)) \n",
    "\n",
    "    for i in range(len(Q)):\n",
    "        words = Q[i].lower().strip().split()\n",
    "        j = 0\n",
    "        for w in words:\n",
    "            x_index[i,j] = word_to_index[w]\n",
    "            j += 1\n",
    "    \n",
    "    return x_index\n",
    "    \n",
    "def EmbeddingLayer(word_to_vec, word_to_index):\n",
    "\n",
    "    EMBED_DIM = word_to_vec['the'].shape[0]\n",
    "    VOCAB_SIZE = len(word_to_index)+1\n",
    "\n",
    "    embed_matrix = np.zeros((VOCAB_SIZE, EMBED_DIM))\n",
    "    for word, index in word_to_index.items():\n",
    "        embed_matrix[index] = word_to_vec[word]\n",
    "    \n",
    "    embedded_layer = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIM, trainable = False, input_length=MAX_WORDS)\n",
    "    embedded_layer.build((None, ))\n",
    "    embedded_layer.set_weights([embed_matrix])\n",
    "    \n",
    "    return embedded_layer\n",
    "\n",
    "def Encode(word_to_vec, word_to_index):\n",
    "    input = Input((50, ), dtype='int32')\n",
    "    \n",
    "    embedding_layer = EmbeddingLayer(word_to_vec, word_to_index)\n",
    "    embeddings = embedding_layer(input)\n",
    "    x = LSTM(128, return_sequences=False)(embeddings)\n",
    "    \n",
    "    model = Model(inputs=input, outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove\n",
      "indices\n",
      "[[386474. 192973. 357266. 352267.  86371.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.]]\n",
      "model\n",
      "[[-0.06840204  0.4617101   0.3132516  -0.38414174 -0.42119378 -0.23517838\n",
      "   0.05085213  0.38185617  0.40476155 -0.508157    0.25222638 -0.45245972\n",
      "   0.2276089   0.02648647  0.12201608 -0.05308213 -0.27630755 -0.19566384\n",
      "   0.3021578   0.04783177  0.1570822   0.25458294 -0.09061718  0.24194878\n",
      "   0.3956943   0.2668201  -0.02871352  0.0446302  -0.31203675 -0.43226755\n",
      "  -0.10929769  0.05444723  0.0776777   0.3943209   0.16269517  0.2933461\n",
      "   0.09048925  0.0637419  -0.5112531  -0.17375949  0.37025145  0.3163457\n",
      "   0.23845947 -0.24136646  0.2347535  -0.39858133 -0.03680982  0.48609912\n",
      "  -0.29266936  0.31643987 -0.13651584  0.34129533  0.43664452 -0.24684624\n",
      "   0.15359758  0.1950781   0.17068109 -0.18083096 -0.03383062 -0.43569186\n",
      "  -0.28409475 -0.49332833 -0.06892151  0.08644017 -0.0903609  -0.31630147\n",
      "  -0.40824994 -0.27176678 -0.18327253 -0.09420006 -0.02409707  0.3306253\n",
      "   0.21500106  0.15170592  0.43588167  0.10445978  0.12901726 -0.23478287\n",
      "   0.10572896 -0.08918255 -0.13963749 -0.22467534 -0.30063903  0.289825\n",
      "   0.07214598  0.24158609  0.07166258 -0.15885867 -0.14055586  0.4251339\n",
      "   0.11911035  0.2534839   0.37608835  0.20824476 -0.2490652   0.41211253\n",
      "  -0.12477403 -0.12434048 -0.09987685 -0.02092715 -0.3787261  -0.15078285\n",
      "   0.18424466 -0.02737366 -0.24847393  0.31029773 -0.34939104  0.25360757\n",
      "  -0.4918512   0.39643556 -0.02094972 -0.21479046 -0.01810552 -0.23098804\n",
      "   0.01348523  0.03100537  0.44509223 -0.10418728  0.20746867  0.0934509\n",
      "  -0.13224034  0.05534847  0.02339562  0.27730134 -0.19212878 -0.00710695\n",
      "   0.25485304  0.08149697]]\n"
     ]
    }
   ],
   "source": [
    "print('glove')\n",
    "params = GetGlove('glove.6B.100d.txt')\n",
    "word_to_vec, word_to_index, index_to_word = params['word_to_vec'],params['word_to_index'],params['index_to_word']\n",
    "   \n",
    "print('indices')\n",
    "ind = QuestionIndices(['Which is the tallest building'],word_to_vec,word_to_index)\n",
    "print(ind)\n",
    "   \n",
    "print('model')\n",
    "model = Encode(word_to_vec,word_to_index)\n",
    "print(model.predict(ind))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddings:\n",
    "    \"\"\"\n",
    "    Class to load and handle the GloVe Word Embeddinfs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocabulary = set()\n",
    "        self.word_to_vec = {}\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "\n",
    "    def load_glove(self, path):\n",
    "        \"\"\"\n",
    "        Loads a pretrained GloVe model\n",
    "        Expects a path to a GloVe pretrained word embeddings file\n",
    "        \"\"\"\n",
    "\n",
    "        with open(path, 'r') as file:\n",
    "            for line in file:\n",
    "                line = line.strip().split()\n",
    "                self.vocabulary.add(line[0])\n",
    "                self.word_to_vec[line[0]] = np.array(line[1:], dtype='float64')\n",
    "    \n",
    "            for x,y in enumerate(sorted(self.vocabulary)):\n",
    "                self.word_to_index[y] = x\n",
    "                self.index_to_word[x] = y\n",
    "        \n",
    "    def in_vocab(self, word):\n",
    "        \"\"\"\n",
    "        Checks if a word is present in the vocabulary\n",
    "        \"\"\"\n",
    "        return (word in self.vocabulary)\n",
    "\n",
    "    def autocorrect(self, wrong_word):\n",
    "        \"\"\"\n",
    "        Attempts to map a wrongly spelt word to the closest one present in the vocabulary.\n",
    "        THIS IS NOT COSINE SIMILARITY. THIS IS AUTOCORRECT.\n",
    "        \"\"\"\n",
    "        closest_ratio = 0.0\n",
    "        closest_word = None\n",
    "        for word in self.vocabulary:\n",
    "            if fuzz.ratio(word,wrong_word) > closest_ratio :\n",
    "                closest_word = word\n",
    "                closest_ratio = fuzz.ratio(word,wrong_word)\n",
    "        return closest_word\n",
    "\n",
    "    def similarity(self, word_1, word_2):\n",
    "        \"\"\"\n",
    "        Returns the cosine similarity of word_1 and word_2\n",
    "        \"\"\"\n",
    "        \n",
    "        assert (self.in_vocab(word_1) and self.in_vocab(word_2))\n",
    "\n",
    "        u = self.word_to_vec[word_1]\n",
    "        v = self.word_to_vec[word_2]\n",
    "\n",
    "        dot = np.sum(u * v)\n",
    "        norm_u = np.sqrt(np.sum(u ** 2))\n",
    "        norm_v = np.sqrt(np.sum(v ** 2))\n",
    "        cosine_similarity = dot / (norm_u * norm_v)\n",
    "\n",
    "        return cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = WordEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.load_glove('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9010410922621748"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.similarity(\"not\", \"n\\'t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove.word_to_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mumbai'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.autocorrect('Mumbai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz, StringMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
